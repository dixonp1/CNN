Convolutional Neural Network.

Convolutional and FullyConneted layers use ReLU activation function.
Each layer is implemented with Adadelta adaptive learning and L2 regularization. 
The FullyConneted layer implements dropout.